{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94f637df-5f89-442e-9359-fec96721c12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "import onnxruntime\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import models, transforms\n",
    "\n",
    "# Load the ONNX model\n",
    "model_path = \"vgg16-7.onnx\"\n",
    "onnx_model = onnx.load(model_path)\n",
    "\n",
    "# Create an ONNX Runtime session\n",
    "ort_session = onnxruntime.InferenceSession(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12fa6be5-1b50-44a0-b672-a0438e7b5563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: onnx in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (1.18.0)\n",
      "Requirement already satisfied: numpy>=1.22 in c:\\programdata\\anaconda3\\lib\\site-packages (from onnx) (1.26.4)\n",
      "Requirement already satisfied: protobuf>=4.25.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from onnx) (4.25.3)\n",
      "Requirement already satisfied: typing_extensions>=4.7.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from onnx) (4.11.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e297bf74-58e4-4b53-bcd4-e31c9effc4d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: opencv-python in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (4.11.0.86)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from opencv-python) (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16bce9c5-ef1e-4b69-96a1-16587d7847ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numba in c:\\programdata\\anaconda3\\lib\\site-packages (0.60.0)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in c:\\programdata\\anaconda3\\lib\\site-packages (from numba) (0.43.0)\n",
      "Requirement already satisfied: numpy<2.1,>=1.22 in c:\\programdata\\anaconda3\\lib\\site-packages (from numba) (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e90e6587-bdd5-4478-b764-7e649e2b108e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numba in c:\\programdata\\anaconda3\\lib\\site-packages (0.60.0)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in c:\\programdata\\anaconda3\\lib\\site-packages (from numba) (0.43.0)\n",
      "Requirement already satisfied: numpy<2.1,>=1.22 in c:\\programdata\\anaconda3\\lib\\site-packages (from numba) (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78ed3831-81da-42a3-a55a-98f4b37082a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 CUDA devices\n",
      "id 0      b'NVIDIA T400 4GB'                              [SUPPORTED]\n",
      "                      Compute Capability: 7.5\n",
      "                           PCI Device ID: 0\n",
      "                              PCI Bus ID: 1\n",
      "                                    UUID: GPU-f748cbed-5ce7-bff1-bd01-ea1a21a5bf6f\n",
      "                                Watchdog: Enabled\n",
      "                            Compute Mode: WDDM\n",
      "             FP32/FP64 Performance Ratio: 32\n",
      "Summary:\n",
      "\t1/1 devices are supported\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "from numba import cuda\n",
    "print(cuda.detect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5573c1c-05cf-485f-82f0-6d72e094a63c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "863d9a03-52ce-4dbc-92c1-ce285bb46bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: onnxruntime-gpu in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (1.22.0)\n",
      "Requirement already satisfied: coloredlogs in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (from onnxruntime-gpu) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (from onnxruntime-gpu) (25.2.10)\n",
      "Requirement already satisfied: numpy>=1.21.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from onnxruntime-gpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from onnxruntime-gpu) (24.1)\n",
      "Requirement already satisfied: protobuf in c:\\programdata\\anaconda3\\lib\\site-packages (from onnxruntime-gpu) (4.25.3)\n",
      "Requirement already satisfied: sympy in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (from onnxruntime-gpu) (1.14.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (from coloredlogs->onnxruntime-gpu) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy->onnxruntime-gpu) (1.3.0)\n",
      "Requirement already satisfied: pyreadline3 in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime-gpu) (3.5.4)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install onnxruntime-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d87deae3-942c-46c9-955c-b79175d82f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA execution provider for GPU acceleration.\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "import onnxruntime\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Load the ONNX model\n",
    "model_path = \"vgg16-7.onnx\"\n",
    "onnx_model = onnx.load(model_path)\n",
    "\n",
    "# Create an ONNX Runtime session with the CUDA execution provider\n",
    "# Check if CUDA is available\n",
    "if 'CUDAExecutionProvider' in onnxruntime.get_available_providers():\n",
    "    print(\"Using CUDA execution provider for GPU acceleration.\")\n",
    "    providers = ['CUDAExecutionProvider']\n",
    "else:\n",
    "    print(\"CUDA is not available, falling back to CPU.\")\n",
    "    providers = ['CPUExecutionProvider']\n",
    "\n",
    "ort_session = onnxruntime.InferenceSession(model_path, providers=providers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f14dac1-4c83-4a9c-a2f2-aa734733da48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: onnx in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (1.18.0)\n",
      "Requirement already satisfied: onnxruntime in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (1.22.0)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: Pillow in c:\\programdata\\anaconda3\\lib\\site-packages (10.4.0)\n",
      "Requirement already satisfied: protobuf>=4.25.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from onnx) (4.25.3)\n",
      "Requirement already satisfied: typing_extensions>=4.7.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from onnx) (4.11.0)\n",
      "Requirement already satisfied: coloredlogs in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (from onnxruntime) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (from onnxruntime) (25.2.10)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from onnxruntime) (24.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (from onnxruntime) (1.14.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (from coloredlogs->onnxruntime) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy->onnxruntime) (1.3.0)\n",
      "Requirement already satisfied: pyreadline3 in c:\\users\\hp\\appdata\\roaming\\python\\python312\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime) (3.5.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install onnx onnxruntime numpy Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4304bc76-655a-45e7-9c43-8db87eba864f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "import onnxruntime\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from onnxruntime.quantization import quantize_static, CalibrationDataReader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83473096-3f66-463b-b855-de3f338a0695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dummy calibration data reader\n",
    "class ImageDataReader(CalibrationDataReader):\n",
    "    def __init__(self, image_paths):\n",
    "        self.image_paths = image_paths\n",
    "        self.data_count = len(image_paths)\n",
    "        \n",
    "    def get_next(self):\n",
    "        if self.data_count > 0:\n",
    "            self.data_count -= 1\n",
    "            image_path = self.image_paths[self.data_count]\n",
    "            # Load and preprocess image\n",
    "            img = Image.open(image_path).convert('RGB').resize((224, 224))\n",
    "            data = np.array(img).astype('float32') / 255.0\n",
    "            data = np.transpose(data, (2, 0, 1))\n",
    "            data = np.expand_dims(data, axis=0)\n",
    "            return {'input.1': data} # The input name might vary, check your model\n",
    "\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "# --- Dummy Calibration Data ---\n",
    "# Create a list of paths to your calibration images\n",
    "# Replace this with your actual image paths\n",
    "calibration_images = [f'dummy_image_{i}.jpg' for i in range(100)] \n",
    "\n",
    "# You need to create some dummy images to run this code\n",
    "# (Not part of the core quantization logic)\n",
    "for img_path in calibration_images:\n",
    "    dummy_img = Image.fromarray(np.random.randint(0, 255, (224, 224, 3), dtype=np.uint8))\n",
    "    dummy_img.save(img_path)\n",
    "\n",
    "calib_reader = ImageDataReader(calibration_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86640e04-2ee5-4fc7-962b-9ccd210bcdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "import onnxruntime\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "def absmax_quantize(weights):\n",
    "    \"\"\"Performs absmax quantization on a weight tensor.\"\"\"\n",
    "    max_val = np.max(np.abs(weights))\n",
    "    scale = 127.0 / max_val\n",
    "    quantized_weights = np.round(weights * scale).astype(np.int8)\n",
    "    return quantized_weights, scale\n",
    "\n",
    "def perform_conv(input_tensor, kernel, bias=None, stride=1, padding=0):\n",
    "    \"\"\"Manually performs a 2D convolution operation.\"\"\"\n",
    "    output_channels, kernel_input_channels, kernel_h, kernel_w = kernel.shape\n",
    "    batch_size, current_input_channels, input_h, input_w = input_tensor.shape\n",
    "    \n",
    "    if kernel_input_channels != current_input_channels:\n",
    "        raise ValueError(f\"Input channels ({current_input_channels}) do not match kernel input channels ({kernel_input_channels}).\")\n",
    "        \n",
    "    # Calculate output dimensions\n",
    "    output_h = (input_h + 2 * padding - kernel_h) // stride + 1\n",
    "    output_w = (input_w + 2 * padding - kernel_w) // stride + 1\n",
    "    \n",
    "    output = np.zeros((batch_size, output_channels, output_h, output_w))\n",
    "    \n",
    "    # Pad the input tensor\n",
    "    padded_input = np.pad(input_tensor, ((0, 0), (0, 0), (padding, padding), (padding, padding)), 'constant', constant_values=0)\n",
    "\n",
    "    for b in range(batch_size):\n",
    "        for c_out in range(output_channels):\n",
    "            for i in range(output_h):\n",
    "                for j in range(output_w):\n",
    "                    h_start = i * stride\n",
    "                    w_start = j * stride\n",
    "                    h_end = h_start + kernel_h\n",
    "                    w_end = w_start + kernel_w\n",
    "                    \n",
    "                    input_patch = padded_input[b, :, h_start:h_end, w_start:w_end]\n",
    "                    kernel_for_channel = kernel[c_out]\n",
    "                    \n",
    "                    conv_result = np.sum(input_patch * kernel_for_channel)\n",
    "                    output[b, c_out, i, j] = conv_result + bias[c_out] if bias is not None else conv_result\n",
    "    \n",
    "    return output\n",
    "\n",
    "def perform_maxpool(input_tensor, kernel_shape, strides, pads):\n",
    "    \"\"\"Manually performs a max pooling operation.\"\"\"\n",
    "    batch_size, channels, input_h, input_w = input_tensor.shape\n",
    "    kernel_h, kernel_w = kernel_shape\n",
    "    stride_h, stride_w = strides\n",
    "    \n",
    "    output_h = (input_h + 2 * pads[0] - kernel_h) // stride_h + 1\n",
    "    output_w = (input_w + 2 * pads[1] - kernel_w) // stride_w + 1\n",
    "    \n",
    "    output = np.zeros((batch_size, channels, output_h, output_w))\n",
    "    \n",
    "    padded_input = np.pad(input_tensor, ((0, 0), (0, 0), (pads[0], pads[2]), (pads[1], pads[3])), 'constant', constant_values=0)\n",
    "    \n",
    "    for b in range(batch_size):\n",
    "        for c in range(channels):\n",
    "            for i in range(output_h):\n",
    "                for j in range(output_w):\n",
    "                    h_start = i * stride_h\n",
    "                    w_start = j * stride_w\n",
    "                    h_end = h_start + kernel_h\n",
    "                    w_end = w_start + kernel_w\n",
    "                    \n",
    "                    input_patch = padded_input[b, c, h_start:h_end, w_start:w_end]\n",
    "                    output[b, c, i, j] = np.max(input_patch)\n",
    "    \n",
    "    return output\n",
    "\n",
    "def perform_gemm(input_tensor, weight, bias):\n",
    "    \"\"\"Manually performs matrix multiplication for fully connected layers.\"\"\"\n",
    "    input_tensor_flat = input_tensor.reshape(input_tensor.shape[0], -1)\n",
    "    return np.matmul(input_tensor_flat, weight.T) + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1d33775-872d-4c70-97bf-a9668c5bfe5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- File Paths ---\n",
    "model_path = \"vgg16-7.onnx\"\n",
    "image_path = \"dummy_image_96.jpg\" # Make sure this image exists in your directory\n",
    "\n",
    "# --- Load the ONNX model and prepare a session ---\n",
    "onnx_model = onnx.load(model_path)\n",
    "ort_session = onnxruntime.InferenceSession(model_path)\n",
    "\n",
    "# --- Prepare Input Image ---\n",
    "img = Image.open(image_path).convert('RGB')\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "input_tensor = transform(img).unsqueeze(0).numpy()\n",
    "\n",
    "# Get model initializers (weights and biases)\n",
    "graph = onnx_model.graph\n",
    "initializers = {init.name: onnx.numpy_helper.to_array(init) for init in graph.initializer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e9aaee8-8b28-4141-844c-be1690f23e5a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_tensor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m current_input \u001b[38;5;241m=\u001b[39m input_tensor\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m graph\u001b[38;5;241m.\u001b[39mnode:\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing layer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnode\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with op_type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnode\u001b[38;5;241m.\u001b[39mop_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'input_tensor' is not defined"
     ]
    }
   ],
   "source": [
    "current_input = input_tensor\n",
    "\n",
    "for node in graph.node:\n",
    "    print(f\"Processing layer: {node.name} with op_type: {node.op_type}\")\n",
    "    \n",
    "    if node.op_type == \"Conv\":\n",
    "        weight_name = node.input[1]\n",
    "        bias_name = node.input[2] if len(node.input) > 2 else None\n",
    "        \n",
    "        weights = initializers[weight_name]\n",
    "        bias = initializers[bias_name] if bias_name else None\n",
    "\n",
    "        # Quantize weights using absmax\n",
    "        quantized_weights, scale = absmax_quantize(weights)\n",
    "        dequantized_weights = quantized_weights / scale\n",
    "        \n",
    "        # Extract layer attributes\n",
    "        strides = [attr.ints[0] for attr in node.attribute if attr.name == \"strides\"][0]\n",
    "        padding = [attr.ints[0] for attr in node.attribute if attr.name == \"pads\"][0]\n",
    "        \n",
    "        current_input = perform_conv(current_input, dequantized_weights, bias, stride=strides, padding=padding)\n",
    "\n",
    "    elif node.op_type == \"Relu\":\n",
    "        current_input = np.maximum(0, current_input)\n",
    "\n",
    "    elif node.op_type == \"MaxPool\":\n",
    "        kernel_shape = [attr.ints for attr in node.attribute if attr.name == \"kernel_shape\"][0]\n",
    "        strides = [attr.ints for attr in node.attribute if attr.name == \"strides\"][0]\n",
    "        pads = [attr.ints for attr in node.attribute if attr.name == \"pads\"][0]\n",
    "        current_input = perform_maxpool(current_input, kernel_shape, strides, pads)\n",
    "\n",
    "    elif node.op_type == \"Gemm\":\n",
    "        weight_name = node.input[1]\n",
    "        bias_name = node.input[2]\n",
    "        \n",
    "        weights = initializers[weight_name]\n",
    "        bias = initializers[bias_name]\n",
    "        \n",
    "        # Quantize weights\n",
    "        quantized_weights, scale = absmax_quantize(weights)\n",
    "        dequantized_weights = quantized_weights / scale\n",
    "        \n",
    "        current_input = perform_gemm(current_input, dequantized_weights, bias)\n",
    "\n",
    "    elif node.op_type == \"Flatten\":\n",
    "        current_input = current_input.reshape(current_input.shape[0], -1)\n",
    "\n",
    "    else:\n",
    "        print(f\"Warning: Unhandled ONNX operator type: {node.op_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "846b304b-97fa-4d8d-9c23-c639aed5c75d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ort_session' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Run the original FP32 model for comparison\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m ort_inputs \u001b[38;5;241m=\u001b[39m {ort_session\u001b[38;5;241m.\u001b[39mget_inputs()[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mname: input_tensor}\n\u001b[0;32m      3\u001b[0m original_output \u001b[38;5;241m=\u001b[39m ort_session\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28;01mNone\u001b[39;00m, ort_inputs)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Get the final predictions\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ort_session' is not defined"
     ]
    }
   ],
   "source": [
    "# Run the original FP32 model for comparison\n",
    "ort_inputs = {ort_session.get_inputs()[0].name: input_tensor}\n",
    "original_output = ort_session.run(None, ort_inputs)[0]\n",
    "\n",
    "# Get the final predictions\n",
    "original_prediction = np.argmax(original_output)\n",
    "quantized_prediction = np.argmax(current_input)\n",
    "\n",
    "print(\"\\n--- Accuracy Check ---\")\n",
    "print(f\"Original model prediction: {original_prediction}\")\n",
    "print(f\"Quantized model prediction: {quantized_prediction}\")\n",
    "\n",
    "# Determine if the top prediction is the same\n",
    "is_accurate = 1 if original_prediction == quantized_prediction else 0\n",
    "print(f\"Prediction accuracy for this image: {is_accurate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f83ea91-720b-4372-9c0c-022ca4297755",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# A module to perform absmax quantization and dequantization\n",
    "class AbsMaxQuantizer(nn.Module):\n",
    "    def __init__(self, layer):\n",
    "        super().__init__()\n",
    "        self.layer = layer\n",
    "        \n",
    "        # Get weights and quantize them on initialization\n",
    "        weights = layer.weight.data\n",
    "        max_val = torch.max(torch.abs(weights))\n",
    "        self.scale = 127.0 / max_val\n",
    "        self.quantized_weights = torch.round(weights * self.scale).to(torch.int8)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # De-quantize the weights for computation\n",
    "        dequantized_weights = self.quantized_weights.to(torch.float32) / self.scale\n",
    "        \n",
    "        # Perform the original layer's forward pass with the de-quantized weights\n",
    "        # We need to handle Conv2d and Linear layers differently\n",
    "        if isinstance(self.layer, nn.Conv2d):\n",
    "            return nn.functional.conv2d(x, dequantized_weights, self.layer.bias, self.layer.stride, self.layer.padding)\n",
    "        elif isinstance(self.layer, nn.Linear):\n",
    "            return nn.functional.linear(x, dequantized_weights, self.layer.bias)\n",
    "        else:\n",
    "            return self.layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b944118a-7c2d-4b78-b2bf-830351e7ddba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Load a pre-trained VGG16 model\n",
    "model_original = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)\n",
    "model_quantized = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)\n",
    "\n",
    "# Move models to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model_original.to(device)\n",
    "model_quantized.to(device)\n",
    "\n",
    "# --- Replace Layers with Quantized Versions ---\n",
    "for name, module in model_quantized.named_modules():\n",
    "    if isinstance(module, (nn.Conv2d, nn.Linear)):\n",
    "        # To handle layers inside Sequential blocks, we need to get the parent module and its index\n",
    "        path = name.split('.')\n",
    "        parent = model_quantized\n",
    "        for i in range(len(path) - 1):\n",
    "            parent = parent._modules[path[i]]\n",
    "        \n",
    "        layer_name = path[-1]\n",
    "        \n",
    "        # Replace the layer with our AbsMaxQuantizer\n",
    "        quantized_layer = AbsMaxQuantizer(module)\n",
    "        setattr(parent, layer_name, quantized_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d8c605d-993f-48df-9502-15574b98a4fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), Output Shape: torch.Size([1, 64, 224, 224])\n",
      "Layer: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), Output Shape: torch.Size([1, 64, 224, 224])\n",
      "Layer: Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), Output Shape: torch.Size([1, 128, 112, 112])\n",
      "Layer: Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), Output Shape: torch.Size([1, 128, 112, 112])\n",
      "Layer: Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), Output Shape: torch.Size([1, 256, 56, 56])\n",
      "Layer: Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), Output Shape: torch.Size([1, 256, 56, 56])\n",
      "Layer: Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), Output Shape: torch.Size([1, 256, 56, 56])\n",
      "Layer: Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), Output Shape: torch.Size([1, 512, 28, 28])\n",
      "Layer: Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), Output Shape: torch.Size([1, 512, 28, 28])\n",
      "Layer: Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), Output Shape: torch.Size([1, 512, 28, 28])\n",
      "Layer: Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), Output Shape: torch.Size([1, 512, 14, 14])\n",
      "Layer: Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), Output Shape: torch.Size([1, 512, 14, 14])\n",
      "Layer: Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), Output Shape: torch.Size([1, 512, 14, 14])\n",
      "Layer: Linear(in_features=25088, out_features=4096, bias=True), Output Shape: torch.Size([1, 4096])\n",
      "Layer: Linear(in_features=4096, out_features=4096, bias=True), Output Shape: torch.Size([1, 4096])\n",
      "Layer: Linear(in_features=4096, out_features=1000, bias=True), Output Shape: torch.Size([1, 1000])\n",
      "\n",
      "--- Accuracy Check ---\n",
      "Original model prediction: 735\n",
      "Quantized model prediction: 735\n",
      "Prediction accuracy for this image: 1\n"
     ]
    }
   ],
   "source": [
    "# --- Prepare Input Image ---\n",
    "image_path = \"dummy_image_96.jpg\" # Replace with your image file path\n",
    "img = Image.open(image_path).convert('RGB')\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "input_tensor = transform(img).unsqueeze(0).to(device)\n",
    "\n",
    "# --- Perform Inference on Both Models ---\n",
    "model_original.eval()\n",
    "model_quantized.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Original model output\n",
    "    output_original = model_original(input_tensor)\n",
    "    \n",
    "    # Quantized model output\n",
    "    output_quantized = model_quantized(input_tensor)\n",
    "\n",
    "# Get predictions\n",
    "_, pred_original = torch.max(output_original, 1)\n",
    "_, pred_quantized = torch.max(output_quantized, 1)\n",
    "\n",
    "print(\"\\n--- Accuracy Check ---\")\n",
    "print(f\"Original model prediction: {pred_original.item()}\")\n",
    "print(f\"Quantized model prediction: {pred_quantized.item()}\")\n",
    "\n",
    "is_accurate = 1 if pred_original.item() == pred_quantized.item() else 0\n",
    "print(f\"Prediction accuracy for this image: {is_accurate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da2a7950-0388-4e62-aa70-124815cebcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A module to perform absmax quantization and dequantization\n",
    "class AbsMaxQuantizer(nn.Module):\n",
    "    def __init__(self, layer):\n",
    "        super().__init__()\n",
    "        self.layer = layer\n",
    "        \n",
    "        # Get weights and quantize them on initialization\n",
    "        weights = layer.weight.data\n",
    "        max_val = torch.max(torch.abs(weights))\n",
    "        self.scale = 127.0 / max_val\n",
    "        self.quantized_weights = torch.round(weights * self.scale).to(torch.int8)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # De-quantize the weights for computation\n",
    "        dequantized_weights = self.quantized_weights.to(torch.float32) / self.scale\n",
    "        \n",
    "        # Perform the original layer's forward pass with the de-quantized weights\n",
    "        if isinstance(self.layer, nn.Conv2d):\n",
    "            output = nn.functional.conv2d(x, dequantized_weights, self.layer.bias, self.layer.stride, self.layer.padding)\n",
    "            print(f\"Layer: {self.layer}, Output Shape: {output.shape}\")\n",
    "            # Optional: Print a sample of the output tensor\n",
    "            # print(f\"Output Sample: {output[0, 0, :2, :2]}\")\n",
    "            return output\n",
    "        elif isinstance(self.layer, nn.Linear):\n",
    "            output = nn.functional.linear(x, dequantized_weights, self.layer.bias)\n",
    "            print(f\"Layer: {self.layer}, Output Shape: {output.shape}\")\n",
    "            # Optional: Print a sample of the output tensor\n",
    "            # print(f\"Output Sample: {output[0, :5]}\")\n",
    "            return output\n",
    "        else:\n",
    "            output = self.layer(x)\n",
    "            print(f\"Layer: {self.layer}, Output Shape: {output.shape}\")\n",
    "            return output\n",
    "\n",
    "# ... (The rest of the code remains the same) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe2a23d4-ea19-4f90-b2a4-2db132c51700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), Output Shape: torch.Size([1, 64, 224, 224])\n",
      "Layer: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), Output Shape: torch.Size([1, 64, 224, 224])\n",
      "Layer: Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), Output Shape: torch.Size([1, 128, 112, 112])\n",
      "Layer: Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), Output Shape: torch.Size([1, 128, 112, 112])\n",
      "Layer: Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), Output Shape: torch.Size([1, 256, 56, 56])\n",
      "Layer: Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), Output Shape: torch.Size([1, 256, 56, 56])\n",
      "Layer: Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), Output Shape: torch.Size([1, 256, 56, 56])\n",
      "Layer: Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), Output Shape: torch.Size([1, 512, 28, 28])\n",
      "Layer: Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), Output Shape: torch.Size([1, 512, 28, 28])\n",
      "Layer: Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), Output Shape: torch.Size([1, 512, 28, 28])\n",
      "Layer: Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), Output Shape: torch.Size([1, 512, 14, 14])\n",
      "Layer: Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), Output Shape: torch.Size([1, 512, 14, 14])\n",
      "Layer: Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), Output Shape: torch.Size([1, 512, 14, 14])\n",
      "Layer: Linear(in_features=25088, out_features=4096, bias=True), Output Shape: torch.Size([1, 4096])\n",
      "Layer: Linear(in_features=4096, out_features=4096, bias=True), Output Shape: torch.Size([1, 4096])\n",
      "Layer: Linear(in_features=4096, out_features=1000, bias=True), Output Shape: torch.Size([1, 1000])\n"
     ]
    }
   ],
   "source": [
    "# --- Perform Inference on Both Models ---\n",
    "model_original.eval()\n",
    "model_quantized.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Original model output\n",
    "    output_original = model_original(input_tensor)\n",
    "    \n",
    "    # Quantized model output\n",
    "    output_quantized = model_quantized(input_tensor)\n",
    "\n",
    "# Apply Softmax to get probabilities\n",
    "probabilities_original = torch.nn.functional.softmax(output_original, dim=1)\n",
    "probabilities_quantized = torch.nn.functional.softmax(output_quantized, dim=1)\n",
    "\n",
    "# Get the top 5 predictions and their probabilities\n",
    "# The 'torch.topk' function returns both the values and their indices\n",
    "top5_prob_orig, top5_idx_orig = torch.topk(probabilities_original, 5)\n",
    "top5_prob_quant, top5_idx_quant = torch.topk(probabilities_quantized, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7f34330d-a92a-48ab-8bad-54f7b6dcc026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load ImageNet Class Labels ---\n",
    "# Make sure you have the 'imagenet_classes.txt' file in your working directory.\n",
    "# You can download it from: https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\n",
    "with open(\"imagenet_classes.txt\") as f:\n",
    "    imagenet_labels = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# --- Prepare Input Image ---\n",
    "image_path = \"dummy_image_96.jpg\" # Replace with your image file path\n",
    "img = Image.open(image_path).convert('RGB')\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "input_tensor = transform(img).unsqueeze(0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d099e224-d418-48d0-bdbb-126c4c84db5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), Output Shape: torch.Size([1, 64, 224, 224])\n",
      "Layer: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), Output Shape: torch.Size([1, 64, 224, 224])\n",
      "Layer: Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), Output Shape: torch.Size([1, 128, 112, 112])\n",
      "Layer: Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), Output Shape: torch.Size([1, 128, 112, 112])\n",
      "Layer: Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), Output Shape: torch.Size([1, 256, 56, 56])\n",
      "Layer: Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), Output Shape: torch.Size([1, 256, 56, 56])\n",
      "Layer: Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), Output Shape: torch.Size([1, 256, 56, 56])\n",
      "Layer: Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), Output Shape: torch.Size([1, 512, 28, 28])\n",
      "Layer: Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), Output Shape: torch.Size([1, 512, 28, 28])\n",
      "Layer: Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), Output Shape: torch.Size([1, 512, 28, 28])\n",
      "Layer: Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), Output Shape: torch.Size([1, 512, 14, 14])\n",
      "Layer: Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), Output Shape: torch.Size([1, 512, 14, 14])\n",
      "Layer: Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), Output Shape: torch.Size([1, 512, 14, 14])\n",
      "Layer: Linear(in_features=25088, out_features=4096, bias=True), Output Shape: torch.Size([1, 4096])\n",
      "Layer: Linear(in_features=4096, out_features=4096, bias=True), Output Shape: torch.Size([1, 4096])\n",
      "Layer: Linear(in_features=4096, out_features=1000, bias=True), Output Shape: torch.Size([1, 1000])\n",
      "\n",
      "--- Original Model Top 5 Predictions ---\n",
      "  1: poncho -> 9.61%\n",
      "  2: stole -> 8.06%\n",
      "  3: wool -> 3.85%\n",
      "  4: coral reef -> 3.30%\n",
      "  5: barn -> 2.16%\n",
      "\n",
      "--- Quantized Model Top 5 Predictions ---\n",
      "  1: poncho -> 20.95%\n",
      "  2: stole -> 11.70%\n",
      "  3: wool -> 5.13%\n",
      "  4: dishrag -> 2.80%\n",
      "  5: coral reef -> 2.23%\n"
     ]
    }
   ],
   "source": [
    "# --- Perform Inference on Both Models ---\n",
    "model_original.eval()\n",
    "model_quantized.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_original = model_original(input_tensor)\n",
    "    output_quantized = model_quantized(input_tensor)\n",
    "\n",
    "# Apply Softmax to get probabilities\n",
    "probabilities_original = torch.nn.functional.softmax(output_original, dim=1)\n",
    "probabilities_quantized = torch.nn.functional.softmax(output_quantized, dim=1)\n",
    "\n",
    "# Get the top 5 predictions and their probabilities\n",
    "top5_prob_orig, top5_idx_orig = torch.topk(probabilities_original, 5)\n",
    "top5_prob_quant, top5_idx_quant = torch.topk(probabilities_quantized, 5)\n",
    "\n",
    "print(\"\\n--- Original Model Top 5 Predictions ---\")\n",
    "for i in range(5):\n",
    "    class_id = top5_idx_orig[0][i].item()\n",
    "    probability = top5_prob_orig[0][i].item()\n",
    "    print(f\"  {i+1}: {imagenet_labels[class_id]} -> {probability * 100:.2f}%\")\n",
    "\n",
    "print(\"\\n--- Quantized Model Top 5 Predictions ---\")\n",
    "for i in range(5):\n",
    "    class_id = top5_idx_quant[0][i].item()\n",
    "    probability = top5_prob_quant[0][i].item()\n",
    "    print(f\"  {i+1}: {imagenet_labels[class_id]} -> {probability * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "17f675c8-a9c5-469c-a63b-7c410e4b298e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), Output Shape: torch.Size([1, 64, 224, 224])\n",
      "Layer: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), Output Shape: torch.Size([1, 64, 224, 224])\n",
      "Layer: Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), Output Shape: torch.Size([1, 128, 112, 112])\n",
      "Layer: Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), Output Shape: torch.Size([1, 128, 112, 112])\n",
      "Layer: Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), Output Shape: torch.Size([1, 256, 56, 56])\n",
      "Layer: Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), Output Shape: torch.Size([1, 256, 56, 56])\n",
      "Layer: Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), Output Shape: torch.Size([1, 256, 56, 56])\n",
      "Layer: Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), Output Shape: torch.Size([1, 512, 28, 28])\n",
      "Layer: Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), Output Shape: torch.Size([1, 512, 28, 28])\n",
      "Layer: Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), Output Shape: torch.Size([1, 512, 28, 28])\n",
      "Layer: Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), Output Shape: torch.Size([1, 512, 14, 14])\n",
      "Layer: Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), Output Shape: torch.Size([1, 512, 14, 14])\n",
      "Layer: Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), Output Shape: torch.Size([1, 512, 14, 14])\n",
      "Layer: Linear(in_features=25088, out_features=4096, bias=True), Output Shape: torch.Size([1, 4096])\n",
      "Layer: Linear(in_features=4096, out_features=4096, bias=True), Output Shape: torch.Size([1, 4096])\n",
      "Layer: Linear(in_features=4096, out_features=1000, bias=True), Output Shape: torch.Size([1, 1000])\n",
      "\n",
      "--- Original Model Top 5 Predictions ---\n",
      "  1: poncho -> 9.61%\n",
      "  2: stole -> 8.06%\n",
      "  3: wool -> 3.85%\n",
      "  4: coral reef -> 3.30%\n",
      "  5: barn -> 2.16%\n",
      "\n",
      "--- Quantized Model Top 5 Predictions ---\n",
      "  1: poncho -> 20.95%\n",
      "  2: stole -> 11.70%\n",
      "  3: wool -> 5.13%\n",
      "  4: dishrag -> 2.80%\n",
      "  5: coral reef -> 2.23%\n",
      "\n",
      "--- Accuracy Check ---\n",
      "Top-1 Prediction Accuracy of Quantized Model: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# --- Perform Inference on Both Models ---\n",
    "model_original.eval()\n",
    "model_quantized.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_original = model_original(input_tensor)\n",
    "    output_quantized = model_quantized(input_tensor)\n",
    "\n",
    "# Apply Softmax to get probabilities\n",
    "probabilities_original = torch.nn.functional.softmax(output_original, dim=1)\n",
    "probabilities_quantized = torch.nn.functional.softmax(output_quantized, dim=1)\n",
    "\n",
    "# Get the top 5 predictions and their probabilities\n",
    "top5_prob_orig, top5_idx_orig = torch.topk(probabilities_original, 5)\n",
    "top5_prob_quant, top5_idx_quant = torch.topk(probabilities_quantized, 5)\n",
    "\n",
    "print(\"\\n--- Original Model Top 5 Predictions ---\")\n",
    "for i in range(5):\n",
    "    class_id = top5_idx_orig[0][i].item()\n",
    "    probability = top5_prob_orig[0][i].item()\n",
    "    print(f\"  {i+1}: {imagenet_labels[class_id]} -> {probability * 100:.2f}%\")\n",
    "\n",
    "print(\"\\n--- Quantized Model Top 5 Predictions ---\")\n",
    "for i in range(5):\n",
    "    class_id = top5_idx_quant[0][i].item()\n",
    "    probability = top5_prob_quant[0][i].item()\n",
    "    print(f\"  {i+1}: {imagenet_labels[class_id]} -> {probability * 100:.2f}%\")\n",
    "\n",
    "# --- Calculate Accuracy ---\n",
    "original_top1_pred = top5_idx_orig[0][0].item()\n",
    "quantized_top1_pred = top5_idx_quant[0][0].item()\n",
    "\n",
    "accuracy = 100.0 if original_top1_pred == quantized_top1_pred else 0.0\n",
    "\n",
    "print(f\"\\n--- Accuracy Check ---\")\n",
    "print(f\"Top-1 Prediction Accuracy of Quantized Model: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "130ce58c-ab94-45e0-b114-07f5359b92d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), Output Shape: torch.Size([1, 64, 224, 224])\n",
      "Layer: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), Output Shape: torch.Size([1, 64, 224, 224])\n",
      "Layer: Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), Output Shape: torch.Size([1, 128, 112, 112])\n",
      "Layer: Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), Output Shape: torch.Size([1, 128, 112, 112])\n",
      "Layer: Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), Output Shape: torch.Size([1, 256, 56, 56])\n",
      "Layer: Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), Output Shape: torch.Size([1, 256, 56, 56])\n",
      "Layer: Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), Output Shape: torch.Size([1, 256, 56, 56])\n",
      "Layer: Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), Output Shape: torch.Size([1, 512, 28, 28])\n",
      "Layer: Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), Output Shape: torch.Size([1, 512, 28, 28])\n",
      "Layer: Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), Output Shape: torch.Size([1, 512, 28, 28])\n",
      "Layer: Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), Output Shape: torch.Size([1, 512, 14, 14])\n",
      "Layer: Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), Output Shape: torch.Size([1, 512, 14, 14])\n",
      "Layer: Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), Output Shape: torch.Size([1, 512, 14, 14])\n",
      "Layer: Linear(in_features=25088, out_features=4096, bias=True), Output Shape: torch.Size([1, 4096])\n",
      "Layer: Linear(in_features=4096, out_features=4096, bias=True), Output Shape: torch.Size([1, 4096])\n",
      "Layer: Linear(in_features=4096, out_features=1000, bias=True), Output Shape: torch.Size([1, 1000])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# --- Assume models and input_tensor are already defined ---\n",
    "# model_original.eval()\n",
    "# model_quantized.eval()\n",
    "# input_tensor = ...\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_original = model_original(input_tensor)\n",
    "    output_quantized = model_quantized(input_tensor)\n",
    "\n",
    "# Apply Softmax to get probabilities\n",
    "probabilities_original = F.softmax(output_original, dim=1)\n",
    "probabilities_quantized = F.softmax(output_quantized, dim=1)\n",
    "\n",
    "# Get the top 5 predictions and their probabilities\n",
    "top5_prob_orig, top5_idx_orig = torch.topk(probabilities_original, 5)\n",
    "top5_prob_quant, top5_idx_quant = torch.topk(probabilities_quantized, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "58f360ca-2b2f-473c-b75a-392476aefaf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Probability Difference for Top 5 Predictions ---\n",
      "Original Top 1: poncho (9.61%)\n",
      "Quantized Prob: 20.95%\n",
      "  --> Difference: 11.34%\n",
      "\n",
      "Original Top 2: stole (8.06%)\n",
      "Quantized Prob: 11.70%\n",
      "  --> Difference: 3.63%\n",
      "\n",
      "Original Top 3: wool (3.85%)\n",
      "Quantized Prob: 5.13%\n",
      "  --> Difference: 1.28%\n",
      "\n",
      "Original Top 4: coral reef (3.30%)\n",
      "Quantized Prob: 2.23%\n",
      "  --> Difference: 1.07%\n",
      "\n",
      "Original Top 5: barn (2.16%)\n",
      "Quantized Prob: 1.00%\n",
      "  --> Difference: 1.16%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Calculate Probability Differences ---\n",
    "print(\"\\n--- Probability Difference for Top 5 Predictions ---\")\n",
    "\n",
    "# Iterate through the top 5 predictions of the original model\n",
    "for i in range(5):\n",
    "    orig_class_id = top5_idx_orig[0][i].item()\n",
    "    orig_prob = top5_prob_orig[0][i].item()\n",
    "    \n",
    "    # Find the probability of this same class in the quantized model's output\n",
    "    quant_prob_for_orig_class = probabilities_quantized[0][orig_class_id].item()\n",
    "    \n",
    "    # Calculate the percentage difference\n",
    "    prob_difference = abs(orig_prob - quant_prob_for_orig_class) * 100\n",
    "    \n",
    "    class_name = imagenet_labels[orig_class_id]\n",
    "\n",
    "    print(f\"Original Top {i+1}: {class_name} ({orig_prob * 100:.2f}%)\")\n",
    "    print(f\"Quantized Prob: {quant_prob_for_orig_class * 100:.2f}%\")\n",
    "    print(f\"  --> Difference: {prob_difference:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cca5720a-786b-4dd9-bb4c-a61e74d27f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import onnxruntime\n",
    "\n",
    "def calculate_top1_consistency(original_model_path, quantized_model_path, validation_dir):\n",
    "    \"\"\"\n",
    "    Calculates the Top-1 prediction consistency between an original and quantized model.\n",
    "\n",
    "    Args:\n",
    "        original_model_path (str): Path to the original ONNX model.\n",
    "        quantized_model_path (str): Path to the quantized ONNX model.\n",
    "        validation_dir (str): Path to the root directory of the validation images.\n",
    "\n",
    "    Returns:\n",
    "        float: The Top-1 prediction consistency as a percentage.\n",
    "    \"\"\"\n",
    "    # Define transformations for validation images\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    # Create a dataset and DataLoader\n",
    "    validation_dataset = datasets.ImageFolder(root=validation_dir, transform=val_transform)\n",
    "    validation_loader = DataLoader(validation_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    # Load both models for inference\n",
    "    ort_session_original = onnxruntime.InferenceSession(original_model_path)\n",
    "    ort_session_quantized = onnxruntime.InferenceSession(quantized_model_path)\n",
    "\n",
    "    correct_matches = 0\n",
    "    total_images = 0\n",
    "\n",
    "    # Iterate through the validation dataset\n",
    "    for images, _ in validation_loader:\n",
    "        # Convert PyTorch tensors to NumPy arrays for ONNX Runtime\n",
    "        images_np = images.numpy()\n",
    "\n",
    "        # Get top-1 prediction from the original model\n",
    "        inputs_orig = {ort_session_original.get_inputs()[0].name: images_np}\n",
    "        output_original = ort_session_original.run(None, inputs_orig)[0]\n",
    "        preds_original = np.argmax(output_original, axis=1)\n",
    "\n",
    "        # Get top-1 prediction from the quantized model\n",
    "        inputs_quant = {ort_session_quantized.get_inputs()[0].name: images_np}\n",
    "        output_quantized = ort_session_quantized.run(None, inputs_quant)[0]\n",
    "        preds_quantized = np.argmax(output_quantized, axis=1)\n",
    "\n",
    "        # Check if predictions match\n",
    "        match_count = (preds_original == preds_quantized).sum()\n",
    "        correct_matches += match_count\n",
    "        total_images += images.size(0)\n",
    "\n",
    "    # Calculate and return the final percentage\n",
    "    consistency_percentage = (correct_matches / total_images) * 100\n",
    "    return consistency_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3a51daf2-c136-46f3-9b08-9f77bdd3cf63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory 'test_data' already exists. Skipping extraction.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# --- Unzip the file first ---\n",
    "zip_file_path = \"testABC.zip\"\n",
    "extracted_folder_name = \"test_data\"\n",
    "\n",
    "# Check if the folder already exists to avoid re-extraction\n",
    "if not os.path.exists(extracted_folder_name):\n",
    "    print(f\"Extracting {zip_file_path} to {extracted_folder_name}...\")\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extracted_folder_name)\n",
    "    print(\"Extraction complete.\")\n",
    "else:\n",
    "    print(f\"Directory '{extracted_folder_name}' already exists. Skipping extraction.\")\n",
    "\n",
    "# --- The rest of your code ---\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Now, point the ImageFolder root to the extracted folder\n",
    "test_dataset = datasets.ImageFolder(\n",
    "    root=extracted_folder_name,  # Use the folder name instead of the zip file\n",
    "    transform=test_transform\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "64344a12-fd68-45c0-8c88-cb8087acb246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 Prediction Consistency: 95.24%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Assuming models are already loaded and on the correct device\n",
    "model_original.eval()\n",
    "model_quantized.eval()\n",
    "\n",
    "consistency_matches = 0\n",
    "original_correct = 0\n",
    "quantized_correct = 0\n",
    "total_images = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Original model prediction\n",
    "        output_original = model_original(images)\n",
    "        _, preds_original = torch.max(output_original, 1)\n",
    "\n",
    "        # Quantized model prediction\n",
    "        output_quantized = model_quantized(images)\n",
    "        _, preds_quantized = torch.max(output_quantized, 1)\n",
    "\n",
    "        # Calculate consistency matches\n",
    "        consistency_matches += (preds_original == preds_quantized).sum().item()\n",
    "\n",
    "        # Calculate accuracy for original model\n",
    "        original_correct += (preds_original == labels).sum().item()\n",
    "\n",
    "        # Calculate accuracy for quantized model\n",
    "        quantized_correct += (preds_quantized == labels).sum().item()\n",
    "\n",
    "        total_images += images.size(0)\n",
    "\n",
    "# Calculate percentages\n",
    "consistency_percentage = (consistency_matches / total_images) * 100\n",
    "\n",
    "\n",
    "print(f\"Top-1 Prediction Consistency: {consistency_percentage:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d1bb88-9b27-4b3c-afb4-e9f4f419f41b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
