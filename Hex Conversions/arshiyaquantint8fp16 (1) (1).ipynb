{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":413443,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":337481,"modelId":358449}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Imports\nimport tensorflow as tf\nimport numpy as np\nfrom tensorflow.keras.applications import VGG16\nfrom tensorflow.keras.applications.vgg16 import preprocess_input, decode_predictions\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\n\nprint(\"Libraries imported.\")\n\n# Load pretrained VGG16 model\nmodel = VGG16(weights='imagenet')\nprint(\"Loaded VGG16 model.\")\n\n# Save weights to npy file (simulate your assignment step)\nweights = {}\nfor layer in model.layers:\n    if layer.get_weights():\n        weights[layer.name] = layer.get_weights()\nnp.save('vgg16_weights.npy', weights)\nprint(\"Weights saved to vgg16_weights.npy\")\n\n# Reload weights into new model instance\nloaded_weights = np.load('vgg16_weights.npy', allow_pickle=True).item()\nmodel_reloaded = VGG16(weights=None)\nfor layer in model_reloaded.layers:\n    if layer.name in loaded_weights:\n        layer.set_weights(loaded_weights[layer.name])\nprint(\"Model reloaded from saved weights.\")\n\n# Convert to FP16 quantized TFLite model\nconverter_fp16 = tf.lite.TFLiteConverter.from_keras_model(model_reloaded)\nconverter_fp16.optimizations = [tf.lite.Optimize.DEFAULT]\nconverter_fp16.target_spec.supported_types = [tf.float16]\ntflite_model_fp16 = converter_fp16.convert()\nwith open('vgg16_fp16.tflite', 'wb') as f:\n    f.write(tflite_model_fp16)\nprint(\"FP16 quantized TFLite model saved as vgg16_fp16.tflite\")\n\n# Representative dataset generator (Improved for better INT8 accuracy)\ndef representative_data_gen():\n    # Simulate real images using random noise here for example\n    # Replace this with loading real images for best accuracy, e.g. from a dataset folder\n    for _ in range(100):\n        img = np.random.rand(1, 224, 224, 3).astype(np.float32)\n        yield [img]\n\n# Convert to INT8 quantized TFLite model\nconverter_int8 = tf.lite.TFLiteConverter.from_keras_model(model_reloaded)\nconverter_int8.optimizations = [tf.lite.Optimize.DEFAULT]\nconverter_int8.representative_dataset = representative_data_gen\nconverter_int8.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\nconverter_int8.inference_input_type = tf.uint8\nconverter_int8.inference_output_type = tf.uint8\ntflite_model_int8 = converter_int8.convert()\nwith open('vgg16_int8.tflite', 'wb') as f:\n    f.write(tflite_model_int8)\nprint(\"INT8 quantized TFLite model saved as vgg16_int8.tflite\")\n\n#  Download and preprocess a sample image for testing inference\nimg_path = tf.keras.utils.get_file('cat.jpg',\n    'https://storage.googleapis.com/download.tensorflow.org/example_images/320px-Felis_catus-cat_on_snow.jpg')\nimg = load_img(img_path, target_size=(224, 224))\ninput_data = preprocess_input(np.expand_dims(img_to_array(img), axis=0))\n\n# Original model prediction\npred = model.predict(input_data)\nprint(\"Original model prediction:\", decode_predictions(pred, top=3)[0])\n\n# FP16 quantized model inference\ninterpreter_fp16 = tf.lite.Interpreter(model_path=\"vgg16_fp16.tflite\")\ninterpreter_fp16.allocate_tensors()\ninput_details_fp16 = interpreter_fp16.get_input_details()\noutput_details_fp16 = interpreter_fp16.get_output_details()\ninterpreter_fp16.set_tensor(input_details_fp16[0]['index'], input_data.astype(np.float32))\ninterpreter_fp16.invoke()\noutput_fp16 = interpreter_fp16.get_tensor(output_details_fp16[0]['index'])\nprint(\"FP16 quantized model prediction:\", decode_predictions(output_fp16, top=3)[0])\n\n# INT8 quantized model inference\ninterpreter_int8 = tf.lite.Interpreter(model_path=\"vgg16_int8.tflite\")\ninterpreter_int8.allocate_tensors()\ninput_details_int8 = interpreter_int8.get_input_details()\noutput_details_int8 = interpreter_int8.get_output_details()\nscale, zero_point = input_details_int8[0]['quantization']\nint8_input = (input_data / scale + zero_point).astype(np.uint8)\ninterpreter_int8.set_tensor(input_details_int8[0]['index'], int8_input)\ninterpreter_int8.invoke()\nint8_output = interpreter_int8.get_tensor(output_details_int8[0]['index'])\nout_scale, out_zero = output_details_int8[0]['quantization']\nfloat_output = (int8_output.astype(np.float32) - out_zero) * out_scale\nprint(\"INT8 quantized model prediction:\", decode_predictions(float_output, top=3)[0])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T20:42:44.776396Z","iopub.execute_input":"2025-05-26T20:42:44.776823Z","iopub.status.idle":"2025-05-26T20:46:13.277706Z","shell.execute_reply.started":"2025-05-26T20:42:44.776797Z","shell.execute_reply":"2025-05-26T20:46:13.276739Z"}},"outputs":[{"name":"stdout","text":"Libraries imported.\nDownloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n\u001b[1m553467096/553467096\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 0us/step\nLoaded VGG16 model.\nWeights saved to vgg16_weights.npy\nModel reloaded from saved weights.\nSaved artifact at '/tmp/tmphc_rq0qb'. The following endpoints are available:\n\n* Endpoint 'serve'\n  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='keras_tensor_138')\nOutput Type:\n  TensorSpec(shape=(None, 1000), dtype=tf.float32, name=None)\nCaptures:\n  139000012905296: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  139000012909520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  139000012910480: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  139000012908560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  139000012909712: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  139000012910096: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  139000012908752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  139000012902608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  139000012907024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  139000012905680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  139000012907216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  139000012906448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  139000012902416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  139000012910288: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  139000012910672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  139000011470608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  139000011472336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  139000011470416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  139000011461200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  139000011472720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  138999554956112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  138999554955152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  138999554955728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  138999554954384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  138999554954960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  138999554953616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  138999554954192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  138999554952848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  138999554953424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  138999554952080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  138999554952656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  138999554950928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1748292191.604473      35 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\nW0000 00:00:1748292191.604515      35 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n","output_type":"stream"},{"name":"stdout","text":"FP16 quantized TFLite model saved as vgg16_fp16.tflite\nSaved artifact at '/tmp/tmpa9tmxh60'. The following endpoints are available:\n\n* Endpoint 'serve'\n  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='keras_tensor_138')\nOutput Type:\n  TensorSpec(shape=(None, 1000), dtype=tf.float32, name=None)\nCaptures:\n  139000012905296: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  139000012909520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  139000012910480: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  139000012908560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  139000012909712: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  139000012910096: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  139000012908752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  139000012902608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  139000012907024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  139000012905680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  139000012907216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  139000012906448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  139000012902416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  139000012910288: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  139000012910672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  139000011470608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  139000011472336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  139000011470416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  139000011461200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  139000011472720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  138999554956112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  138999554955152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  138999554955728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  138999554954384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  138999554954960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  138999554953616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  138999554954192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  138999554952848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  138999554953424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  138999554952080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  138999554952656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  138999554950928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/tensorflow/lite/python/convert.py:997: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n  warnings.warn(\nW0000 00:00:1748292237.131589      35 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\nW0000 00:00:1748292237.131626      35 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\nfully_quantize: 0, inference_type: 6, input_inference_type: UINT8, output_inference_type: UINT8\n","output_type":"stream"},{"name":"stdout","text":"INT8 quantized TFLite model saved as vgg16_int8.tflite\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 594ms/step\nOriginal model prediction: [('n02127052', 'lynx', 0.5816908), ('n02114855', 'coyote', 0.36767617), ('n02114367', 'timber_wolf', 0.009691837)]\nFP16 quantized model prediction: [('n02127052', 'lynx', 0.5817597), ('n02114855', 'coyote', 0.3675262), ('n02114367', 'timber_wolf', 0.009713426)]\nINT8 quantized model prediction: [('n15075141', 'toilet_tissue', 0.03125), ('n03788365', 'mosquito_net', 0.03125), ('n04209239', 'shower_curtain', 0.03125)]\n","output_type":"stream"}],"execution_count":16}]}